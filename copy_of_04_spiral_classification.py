# -*- coding: utf-8 -*-
"""Copy of 04-spiral_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bH5aXe6tRxONUNzY8yEdm0AbAlrIjNn2

### Create the data
"""

# Mount google drive
from google.colab import drive
drive.mount('/content/drive')

import sys
import os
os.chdir('/content/drive/MyDrive/ColabNotebooks/DeepLearningCourseNYU/')

import random
import torch
from torch import nn, optim
import math
from IPython import display

from res.plot_lib import plot_data, plot_model, set_default
set_default()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

seed = 12345
random.seed(seed)
torch.manual_seed(seed)
N = 1000  # num_samples_per_class
D = 2     # dimensions
C = 3     # num_classes
H = 100   # num_hidden_units

X = torch.zeros(N * C, D).to(device) # inputs
y = torch.zeros(N * C, dtype=torch.long).to(device) # labels
for c in range(C):
    index = 0
    t = torch.linspace(0, 1, N)
    # When c = 0 and t = 0: start of linspace
    # When c = 0 and t = 1: end of linpace
    # This inner_var is for the formula inside sin() and cos() like sin(inner_var) and cos(inner_Var)
    inner_var = torch.linspace(
        # When t = 0
        (2 * math.pi / C) * (c),
        # When t = 1
        (2 * math.pi / C) * (2 + c), N) + \
        torch.randn(N) * 0.2 # add some noise
    
    for ix in range(N * c, N * (c + 1)):
        X[ix] = t[index] * torch.FloatTensor((math.sin(inner_var[index]), math.cos(inner_var[index])))
        y[ix] = c
        index += 1

print("Shapes:")
print("X:", tuple(X.size()))
print("y:", tuple(y.size()))
print(X) # data (2D points)
print(y) # data labels

# visualise the data
plot_data(X, y)

"""### Linear model"""

learning_rate = 1e-3
lambda_l2     = 1e-5

# nn package to create our linear model
# each Linear module has a weight and bias
model = nn.Sequential(
        nn.Linear(D, H),
        nn.Linear(H, C))
model.to(device) #Convert to CUDA

# nn package also has different loss functions.
# we use cross entropy loss for our classification task
criterion = torch.nn.CrossEntropyLoss()

# we use the optim package to apply
# stochastic gradient descent for our parameter updates
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2

# Training
for t in range(1000):
    
    # Feed forward to get the logits
    y_pred = model(X)
    
    # Compute the loss and accuracy
    loss = criterion(y_pred, y)
    score, predicted = torch.max(y_pred, 1)
    acc = (y == predicted).sum().float() / len(y)
    print("[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f" % (t, loss.item(), acc))
    display.clear_output(wait=True)
    
    # zero the gradients before running
    # the backward pass.
    optimizer.zero_grad()
    
    # Backward pass to compute the gradient
    # of loss w.r.t our learnable params. 
    loss.backward()
    
    # Update params
    optimizer.step()

# Plot trained model
print(model)
plot_model(X, y, model)

"""### Two-layered network"""

learning_rate = 1e-3
lambda_l2     = 1e-5

# nn package to create our linear model
# each Linear module has a weight and bias

model = nn.Sequential(
    nn.Linear(D, H),
    nn.ReLU(),
    nn.Linear(H, C))
model.to(device)

# nn package also has different loss functions.
# we use cross entropy loss for our classification task
criterion = torch.nn.CrossEntropyLoss()

# we use the optim package to apply
# ADAM for our parameter updates
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2

# e = 1.  # plotting purpose

# Training
for t in range(2000):
    
    # Feed forward to get the logits
    y_pred = model(X)
    
    # Compute the loss and accuracy
    loss = criterion(y_pred, y)
    score, predicted = torch.max(y_pred, 1)
    acc = (y == predicted).sum().float() / len(y)
    print("[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f" % (t, loss.item(), acc))
    display.clear_output(wait=True)
    
    # zero the gradients before running
    # the backward pass.
    optimizer.zero_grad()
    
    # Backward pass to compute the gradient
    # of loss w.r.t our learnable params. 
    loss.backward()
    
    # Update params
    optimizer.step()

# Plot trained model
print(model)
plot_model(X, y, model)